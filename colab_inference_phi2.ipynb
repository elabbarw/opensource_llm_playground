{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "25gYtZty2WEb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers[torch]\n",
        "!pip install peft\n",
        "!pip install datasets\n",
        "!pip install bitsandbytes\n",
        "!pip install tqdm\n",
        "!pip install einops\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "x9MxH2Ek2X52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")"
      ],
      "metadata": {
        "id": "HhWgXkG62YA_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name=\"microsoft/phi-2\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")"
      ],
      "metadata": {
        "id": "XAhMaAc22YDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "RWSXH1oB2YGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base_model,\n",
        "                                  \"USERNAME/MODEL\",\n",
        "                                  use_auth_token=True)"
      ],
      "metadata": {
        "id": "Lifwemhs2YIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(entry):\n",
        "    return f\"Instruct:{entry}\\nOutput:\""
      ],
      "metadata": {
        "id": "HkgYCeNM2fhl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(entry):\n",
        "    model_input = tokenizer(\n",
        "        make_prompt(entry),\n",
        "        return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    input_length = len(model_input['input_ids'][0])\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        full_tokens = model.generate(**model_input, max_new_tokens=100)[0]\n",
        "        decoded_tokens = tokenizer.decode(full_tokens[input_length:], skip_special_tokens=True)\n",
        "    return decoded_tokens"
      ],
      "metadata": {
        "id": "bVJTKJA82fkK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(\"Hello! Are you doing?\")"
      ],
      "metadata": {
        "id": "Ma2tL76i2fmx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}