{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install transformers\n",
    "%pip -q install sentencepiece\n",
    "%pip -q install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Hugging Face model_path\n",
    "model_path = 'psmathur/orca_mini_3b'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "\n",
    "\n",
    "#generate text function\n",
    "def generate_text(system, instruction, input=None):\n",
    "\n",
    "    if input:\n",
    "        prompt = f\"### System:\\n{system}\\n\\n### User:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### System:\\n{system}\\n\\n### User:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.LongTensor(tokens).unsqueeze(0)\n",
    "    tokens = tokens.to('cuda')\n",
    "\n",
    "    instance = {'input_ids': tokens,'top_p': 1.0, 'temperature':0.7, 'generate_len': 1024, 'top_k': 50}\n",
    "\n",
    "    length = len(tokens[0])\n",
    "    with torch.no_grad():\n",
    "        rest = model.generate(\n",
    "            input_ids=tokens,\n",
    "            max_length=length+instance['generate_len'],\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "            top_p=instance['top_p'],\n",
    "            temperature=instance['temperature'],\n",
    "            top_k=instance['top_k']\n",
    "        )\n",
    "    output = rest[0][length:]\n",
    "    string = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    return f'[!] Response: {string}'\n",
    "\n",
    "system = 'You are an AI assistant that follows instruction extremely well. Help as much as you can.'\n",
    "\n",
    "instruction = '''\n",
    "  I really hated the show! -- negative\\n\n",
    "  I somewhat liked it -- Positive\\n\n",
    "  Meh, i am not really sure about that show -- neutral\\n\n",
    "  It was ok. I guess.. --\n",
    "'''\n",
    "print(\"Few-shot: \\n\"+str(generate_text(system, instruction)))\n",
    "\n",
    "\n",
    "instruction = '''\n",
    "Which is a faster way to get home?\\n\n",
    "Option 1-Take an 10 minutes bus, then an 40 minute bus, and finally a 10 minute train.\\n\n",
    "Option 2-Take a 90 minutes train, then a 45 minute bike ride, and finally a 10 minute bus.\\n\n",
    "Option 1 will take 10+40+10 = 60 minutes.\\n\n",
    "Option 2 will take 90+45+10 = 145 minutes.\\n\n",
    "Since Option 1 takes 60 minutes and Option 2 takes 145 minutes, Option 1 is less than option 2 which makes it faster.\\n\n",
    "\n",
    "Which is a faster way to get to work?\\n\n",
    "Option 1 - Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.\\n\n",
    "Option 2 - Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.\\n\n",
    "Take a deep breath and think step by step. Which option is faster?\\n\n",
    "'''\n",
    "print(\"Few-Shot COT: \\n\"+str(generate_text(system, instruction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
